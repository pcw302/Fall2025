{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfdf2f8",
   "metadata": {},
   "source": [
    "Summarize the problem:\n",
    "    The chanallanged being solved is creating a reinforcement learning agent that can balance a pole on a cart. Without direct instruction the agent must learn to move the pole left or right at the \n",
    "    correct times to keep the pole balanced. It must learn this through trial and error, receiving rewards for successful balancing and penalties for failures. Part of the difficulty lies in the amount of reward and penalty the agent receives, as well as the complexity of the environment itself.\n",
    "    given for each action. In total we want to keep the pole upright for 200 steps to consider the environment solved.\n",
    "\n",
    "Understanding the type and nature of RL to be used:\n",
    "    This model uses unsupervised reinforcement learning with a trial and error approach. The agent is given a very basic policy of, if the pole is leaning left, move left if its leaning right move right.\n",
    "    This decision is then returned back the the model and it returns a reward or penalty based on the action taken. \n",
    "    Only a basic policy hsa been implimented, so the model is not actually learning here, it is simply just following a\n",
    "    set of instructions. This is why you see the model can only hold hold the pole up for 63 steps, well short of the 200.\n",
    "    \n",
    "    Later in the notebook a more complex policy is implimented using Q-learning, \n",
    "    which allows the model to learn from its mistakes and improve over time. A Deep Q-Network\n",
    "    is intitalized to allow the model to learn a better policy from scratch by interacting with the envirnemnt\n",
    "    then using the rewards and penalties to update its Q-values and improve its decision-making process.\n",
    "    \n",
    "    The policy gradient section is used to make good descions more probable and bad descions less probable. \n",
    "    This is the renforcement part of RL learning, the algorithm renforces good descisions by making them more likely\n",
    "    to be chosen in the future. This is done by the discount and normalize reawrds function, which makes it so that\n",
    "    early rewards are the sum off all rewards that came before it, but future rewards are discounted by a set rate\n",
    "    to allow the model to correctly correlate its sucsess with its earlier actions. They are also normalized to \n",
    "    add stability to the learning process.\n",
    "\n",
    "    \n",
    "    We a that the model is initalized with an input layer that takes in the 4 state variables\n",
    "    Along with that a \"replay buffer\" which is basically an array with previous experinces that the model\n",
    "    learns from a randomly sampled bunch to endssure it is not overfitting.\n",
    "    The model is then trained in a loop of 600 \"episodes\" (trials) where it plays a full attempt of the game.\n",
    "    In each episode, it uses a epsilon-greedy policy, so that it balances learning new decisions and making the best decision\n",
    "    Basically, either the model makes a random decision or it makes the best decision based on its current knowledge.\n",
    "    The tuple that stores the information about that step is then returned to the replay_buffer, and if the buffer is\n",
    "    large enough then the model samples a mini-batch of experiences to learn from. The model then updates its Q-values\n",
    "    based on the rewards and penalties received.\n",
    "    \n",
    "    The last 3 versions of the DQN all work on the problem of the model having to do 2 things at once\n",
    "    it has to predict the Q-values for the current state and action as well as for the next state. This means that\n",
    "    as the model adjusts its weights, the target q-values also shift. The Double DQN, Dueling Double DQN and\n",
    "    Fixed Q-Value Targets all attempt to solve this problem in different ways, this the goal of stabilizing the convergence\n",
    "    of the model to an optimal policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da289e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fd0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "obs, info = env.reset(seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da96f58c",
   "metadata": {},
   "source": [
    "The Policy in the given notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e6d5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9521aedd",
   "metadata": {},
   "source": [
    "My policy: Adds the angluar velocity and the angle together to determine if the pole will still be moving in the same direction or not in the next step and adjust accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b8b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_policy(obs):\n",
    "    angle = obs[2]\n",
    "    angular_velocity = obs[3]\n",
    "    if angle + angular_velocity < 0: #Because right is the positive direction, if the sum is negative its falling left\n",
    "        return 0  # left\n",
    "    else:\n",
    "        return 1  # right \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7bcd1",
   "metadata": {},
   "source": [
    "Testing the given policy \n",
    "\n",
    "The testing code it the same as in the given notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0927f951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Basic Policy ---\n",
      "Mean total rewards over 500 episodes: 41.698\n",
      "Standard deviation: 8.389445512070509\n",
      "Minimum rewards in an episode: 24.0\n",
      "Maximum rewards in an episode: 63.0\n"
     ]
    }
   ],
   "source": [
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    totals.append(episode_rewards)\n",
    "print(\"--- Basic Policy ---\")\n",
    "print(f\"Mean total rewards over 500 episodes: {np.mean(totals)}\")\n",
    "print(f\"Standard deviation: {np.std(totals)}\")\n",
    "print(f\"Minimum rewards in an episode: {np.min(totals)}\")\n",
    "print(f\"Maximum rewards in an episode: {np.max(totals)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9befd29e",
   "metadata": {},
   "source": [
    "Testing my Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe93ef75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- My Policy ---\n",
      "Mean total rewards over 500 episodes: 200.0\n",
      "Standard deviation: 0.0\n",
      "Minimum rewards in an episode: 200.0\n",
      "Maximum rewards in an episode: 200.0\n"
     ]
    }
   ],
   "source": [
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    for step in range(200):\n",
    "        action = my_policy(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    totals.append(episode_rewards)\n",
    "print(\"--- My Policy ---\")\n",
    "print(f\"Mean total rewards over 500 episodes: {np.mean(totals)}\")\n",
    "print(f\"Standard deviation: {np.std(totals)}\")\n",
    "print(f\"Minimum rewards in an episode: {np.min(totals)}\")\n",
    "print(f\"Maximum rewards in an episode: {np.max(totals)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
